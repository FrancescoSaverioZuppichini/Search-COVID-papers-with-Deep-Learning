{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search COVID papers with Deep Learning\n",
    "*Transformers + Elastic Search = ❤️*\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/medium/images/cl.gif?raw=true)\n",
    "\n",
    "Today we are going to build a semantic browser using deep learning to search in more than 50k papers about the recent COVID-19 disease.  \n",
    "\n",
    "All the code is on my GitHub [repo](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning). While a live version of this article is [here](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/tutorial.ipynb)\n",
    "\n",
    "The key idea is to encode each paper in a vector representing its semantic content and then search using cosine similarity between a query and all the encoded documents. This is the same process used by image browsers (e.g. Google Images) to search for similar images. \n",
    "\n",
    "So, our puzzle is composed of three pieces: data, a mapping from papers to vectors and a way to search.\n",
    "\n",
    "Most of the work is based on [this project](https://github.com/gsarti/covid-papers-browser) in which I am working with students from the Universita of Trieste (Italy). A live demo is available [here](http://covidbrowser.areasciencepark.it/).\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Everything starts with the data. We will use this [dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) from Kaggle. A list of of over 57,000 scholarly articles prepared by the White House and a coalition of leading research groups. Actually, the only file we need is `metadata.csv` that contains information about the papers and the full text of the abstract. You need to store the file inside `./dataset`.\n",
    "\n",
    "Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zjufx4fo</td>\n",
       "      <td>b2897e1277f56641193a6db73825f707eed3e4c9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Sequence requirements for RNA strand transfer ...</td>\n",
       "      <td>10.1093/emboj/20.24.7220</td>\n",
       "      <td>PMC125340</td>\n",
       "      <td>11742998.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>Nidovirus subgenomic mRNAs contain a leader se...</td>\n",
       "      <td>2001-12-17</td>\n",
       "      <td>Pasternak, Alexander O.; van den Born, Erwin; ...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125340?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ymceytj3</td>\n",
       "      <td>e3d0d482ebd9a8ba81c254cc433f314142e72174</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Crystal structure of murine sCEACAM1a[1,4]: a ...</td>\n",
       "      <td>10.1093/emboj/21.9.2076</td>\n",
       "      <td>PMC125375</td>\n",
       "      <td>11980704.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>CEACAM1 is a member of the carcinoembryonic an...</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125375?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wzj2glte</td>\n",
       "      <td>00b1d99e70f779eb4ede50059db469c65e8c1469</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Synthesis of a novel hepatitis C virus protein...</td>\n",
       "      <td>10.1093/emboj/20.14.3840</td>\n",
       "      <td>PMC125543</td>\n",
       "      <td>11447125.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Hepatitis C virus (HCV) is an important human ...</td>\n",
       "      <td>2001-07-16</td>\n",
       "      <td>Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...</td>\n",
       "      <td>EMBO J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2sfqsfm1</td>\n",
       "      <td>cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Structure of coronavirus main proteinase revea...</td>\n",
       "      <td>10.1093/emboj/cdf327</td>\n",
       "      <td>PMC126080</td>\n",
       "      <td>12093723.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>The key enzyme in coronavirus polyprotein proc...</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Anand, Kanchan; Palm, Gottfried J.; Mesters, J...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc126080?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i0zym7iq</td>\n",
       "      <td>dde02f11923815e6a16a31dd6298c46b109c5dfa</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discontinuous and non-discontinuous subgenomic...</td>\n",
       "      <td>10.1093/emboj/cdf635</td>\n",
       "      <td>PMC136939</td>\n",
       "      <td>12456663.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>Arteri-, corona-, toro- and roniviruses are ev...</td>\n",
       "      <td>2002-12-01</td>\n",
       "      <td>van Vliet, A.L.W.; Smits, S.L.; Rottier, P.J.M...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc136939?pdf=re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  zjufx4fo  b2897e1277f56641193a6db73825f707eed3e4c9      PMC   \n",
       "1  ymceytj3  e3d0d482ebd9a8ba81c254cc433f314142e72174      PMC   \n",
       "2  wzj2glte  00b1d99e70f779eb4ede50059db469c65e8c1469      PMC   \n",
       "3  2sfqsfm1  cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0      PMC   \n",
       "4  i0zym7iq  dde02f11923815e6a16a31dd6298c46b109c5dfa      PMC   \n",
       "\n",
       "                                               title  \\\n",
       "0  Sequence requirements for RNA strand transfer ...   \n",
       "1  Crystal structure of murine sCEACAM1a[1,4]: a ...   \n",
       "2  Synthesis of a novel hepatitis C virus protein...   \n",
       "3  Structure of coronavirus main proteinase revea...   \n",
       "4  Discontinuous and non-discontinuous subgenomic...   \n",
       "\n",
       "                        doi      pmcid   pubmed_id license  \\\n",
       "0  10.1093/emboj/20.24.7220  PMC125340  11742998.0     unk   \n",
       "1   10.1093/emboj/21.9.2076  PMC125375  11980704.0     unk   \n",
       "2  10.1093/emboj/20.14.3840  PMC125543  11447125.0   no-cc   \n",
       "3      10.1093/emboj/cdf327  PMC126080  12093723.0     unk   \n",
       "4      10.1093/emboj/cdf635  PMC136939  12456663.0     unk   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Nidovirus subgenomic mRNAs contain a leader se...   2001-12-17   \n",
       "1  CEACAM1 is a member of the carcinoembryonic an...   2002-05-01   \n",
       "2  Hepatitis C virus (HCV) is an important human ...   2001-07-16   \n",
       "3  The key enzyme in coronavirus polyprotein proc...   2002-07-01   \n",
       "4  Arteri-, corona-, toro- and roniviruses are ev...   2002-12-01   \n",
       "\n",
       "                                             authors           journal  \\\n",
       "0  Pasternak, Alexander O.; van den Born, Erwin; ...  The EMBO Journal   \n",
       "1  Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...  The EMBO Journal   \n",
       "2  Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...            EMBO J   \n",
       "3  Anand, Kanchan; Palm, Gottfried J.; Mesters, J...  The EMBO Journal   \n",
       "4  van Vliet, A.L.W.; Smits, S.L.; Rottier, P.J.M...  The EMBO Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN           True   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  http://europepmc.org/articles/pmc125340?pdf=re...  \n",
       "1  http://europepmc.org/articles/pmc125375?pdf=re...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  http://europepmc.org/articles/pmc126080?pdf=re...  \n",
       "4  http://europepmc.org/articles/pmc136939?pdf=re...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Project import Project\n",
    "# Project holds all the paths\n",
    "pr = Project()\n",
    "\n",
    "df = pd.read_csv(pr.data_dir / 'metadata.csv')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a lot of information. We are obviously interested in the text columns. Working with pandas is not ideal, so let's create a `Dataset`. This will allow us to later create a `DataLoader` to perform batch-wise encoding. If you are not familiar with the Pytorch data loading ecosystem you can read more about [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CovidPapersDataset(Dataset):\n",
    "    FILTER_TITLES = ['Index', 'Subject Index', 'Subject index', 'Author index', 'Contents', \n",
    "    'Articles of Significant Interest Selected from This Issue by the Editors',\n",
    "    'Information for Authors', 'Graphical contents list', 'Table of Contents',\n",
    "    'In brief', 'Preface', 'Editorial Board', 'Author Index', 'Volume Contents',\n",
    "    'Research brief', 'Abstracts', 'Keyword index', 'In This Issue', 'Department of Error',\n",
    "    'Contents list', 'Highlights of this issue', 'Abbreviations', 'Introduction',\n",
    "    'Cumulative Index', 'Positions available', 'Index of Authors', 'Editorial',\n",
    "    'Journal Watch', 'QUIZ CORNER', 'Foreword', 'Table of contents', 'Quiz Corner',\n",
    "    'INDEX', 'Bibliography of the current world literature', 'Index of Subjects',\n",
    "    '60 Seconds', 'Contributors', 'Public Health Watch', 'Commentary',\n",
    "    'Chapter 1 Introduction', 'Facts and ideas from anywhere', 'Erratum',\n",
    "    'Contents of Volume', 'Patent reports', 'Oral presentations', 'Abkürzungen',\n",
    "    'Abstracts cont.', 'Related elsevier virology titles contents alert', 'Keyword Index',\n",
    "    'Volume contents', 'Articles of Significant Interest in This Issue', 'Appendix', \n",
    "    'Abkürzungsverzeichnis', 'List of Abbreviations', 'Editorial Board and Contents',\n",
    "    'Instructions for Authors', 'Corrections', 'II. Sachverzeichnis', '1 Introduction',\n",
    "    'List of abbreviations', 'Response', 'Feedback', 'Poster Sessions', 'News Briefs',\n",
    "    'Commentary on the Feature Article', 'Papers to Appear in Forthcoming Issues', 'TOC',\n",
    "    'Glossary', 'Letter from the editor', 'Croup', 'Acronyms and Abbreviations',\n",
    "    'Highlights', 'Forthcoming papers', 'Poster presentations', 'Authors',\n",
    "    'Journal Roundup', 'Index of authors', 'Table des mots-clés', 'Posters',\n",
    "    'Cumulative Index 2004', 'A Message from the Editor', 'Contents and Editorial Board',\n",
    "    'SUBJECT INDEX', 'Contents page 1']\n",
    "    # Abstracts that should be treated as missing abstract\n",
    "    FILTER_ABSTRACTS = ['Unknown', '[Image: see text]']\n",
    "\n",
    "    def __init__(self, df, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.df = df\n",
    "        self.df = self.df[['title', 'authors', 'abstract', 'url', 'pubmed_id']]\n",
    "        self.df.loc[:,'title'].fillna('', inplace = True)\n",
    "        self.df.loc[:,'title'] = df.title.apply( lambda x: '' if x in self.FILTER_TITLES else x)\n",
    "        self.df.loc[:,'abstract'] = df.abstract.apply( lambda x: '' if x in self.FILTER_ABSTRACTS else x)\n",
    "        self.df = self.df[self.df['abstract'].notna()]\n",
    "        self.df = self.df[self.df.abstract != '']\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.df = self.df.fillna(0)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        self.df.loc[idx:, 'title_abstract'] = f\"{row['title']} {row['abstract']}\"\n",
    "        return  self.df.loc[idx].to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_path(cls, path, *args, **kwargs):\n",
    "        df = pd.read_csv(path)\n",
    "        return cls(df=df, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order, I have subclassed torch.utils.data.Dataset to create a custom dataset. The dataset is expecting a dataframe as input from which we kept only the interesting columns. Then, we removed some of the rows where the abstract and title columns matched one of the \"junk\" words in FILTER_TITLE and FILTER_ABSTRACT respectively. This is done because articles were scrapped in an automatic fashion, and many have irrelevant entries instead of title/abstract information.\n",
    "\n",
    "The dataset returns a dictionary since `pd.DataFrame` is not a supported type in pytorch. To give our search engine more context, we merge the `title` and the `abstract` together, the result is stored in the `title_abstract` key.\n",
    "\n",
    "We can now call the dataset and see if everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/envs/dl/lib/python3.7/site-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/francesco/anaconda3/envs/dl/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sequence requirements for RNA strand transfer during nidovirus discontinuous subgenomic RNA synthesis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = CovidPapersDataset.from_path(pr.data_dir / 'metadata.csv')\n",
    "\n",
    "ds[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed\n",
    "\n",
    "Now, we need a way to create a vector (*embedding*) from the data. We define a class `Embedder` that loads automatically a model from [HuggingFace's `transformers`](https://github.com/huggingface/transformers) using the [sentence_transformers](https://github.com/UKPLab/sentence-transformers) library.\n",
    "\n",
    "The model of choice is [gsarti/biobert-nli](https://huggingface.co/gsarti/biobert-nli) a [BioBERT](https://github.com/dmis-lab/biobert) model fine-tuned on the [SNLI](https://nlp.stanford.edu/projects/snli/) and the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) to produce [universal sentence embeddings](https://www.aclweb.org/anthology/D17-1070/). The finetuning was made by [Gabriele Sarti](https://www.gsarti.com), the code to reproduce it is available [here](https://github.com/gsarti/covid-papers-browser/blob/master/scripts/finetune_nli.py).\n",
    "\n",
    "BioBERT is especially fit for our dataset since it was originally trained on biomedical scientific publications. So, it should create better context-aware embeddings given the similarity with our data.\n",
    "\n",
    "Under the hood, the model first tokenizes the input string in tokens,  then it creates one vector for each one of them. So, if we have `N` tokens in one paper we will get a `[N, 768]` vector (note that a token often corresponds to a word piece, read more about tokenization strategies [here](https://www.thoughtvector.io/blog/subword-tokenization/). Thus, if two papers have a different word size, we will have two vectors with two different first dimensions. This is a problem since we need to compare them to search.\n",
    "\n",
    "To get a fixed embed for each paper, we apply average pooling. This methodology  computes the average of each word and outputs a fixed-size vector of dims `[1, 768]`\n",
    "\n",
    "So, let's code an `Embedder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "@dataclass\n",
    "class Embedder:\n",
    "    name: str = 'gsarti/scibert-nli'\n",
    "    max_seq_length: int  = 128\n",
    "    do_lower_case: bool  = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        word_embedding_model = models.BERT(\n",
    "            'gsarti/biobert-nli',\n",
    "            max_seq_length=128,\n",
    "            do_lower_case=True\n",
    "        )\n",
    "        # apply pooling to get one fixed vector\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                pooling_mode_mean_tokens=True,\n",
    "                pooling_mode_cls_token=False,\n",
    "                pooling_mode_max_tokens=False\n",
    "            )\n",
    "    \n",
    "        self.model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        return self.model.encode(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try our embedder on a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = Embedder()\n",
    "\n",
    "emb = embedder(ds[0]['title_abstract'])\n",
    "\n",
    "emb[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! We encoded one paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Okay, we know how to embed each paper, but how can we search in the data using a query? Assuming we have embedded **all** the papers we could also **embed the query** and compute the cosine similarity between the query and all the embeddings. Then, we can show the results sorted by the distance (score). Intuitively, the closer they are in the embedding space to the query the more context similarity they share. \n",
    "\n",
    "But, how? First, we need a proper way to manage the data and to run the cosine similarity fast enough. Fortunately, Elastic Search comes to the rescue!\n",
    "\n",
    "### Elastic Search\n",
    "\n",
    "[Elastic Search](https://www.elastic.co/) is a database with one goal, yes you guessed right: search. We will first store all the embedding in elastic and then use its API to perform the searching. If you are lazy like me you can [install elastic search with docker](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html)\n",
    "\n",
    "```\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n",
    "docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n",
    "\n",
    "```\n",
    "\n",
    "Perfect. The next step is to store the embeddings and the papers' information on elastic search. It is a very straightforward process. We have to need to create an `index` (a new database) and then build one entry for each paper.\n",
    "\n",
    "To create an `index` we need to describe for elastic what we wish to store. In our case:\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"true\",\n",
    "        \"_source\": {\n",
    "            \"enabled\": \"true\"\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            ... all other properties (columns of the datafarme)\n",
    "            \"embed\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 768\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "You can read more about the index creation on the elastic search [doc](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html). The last entry defines the `embed` field as a dense vector with `768`. This is indeed our embedding. \n",
    "For convenience, I have stored the configuration in a `.json` file and created a class named `ElasticSearchProvider` to handle the storing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "@dataclass\n",
    "class ElasticSearchProvider:\n",
    "    index_file: dict\n",
    "    client: Elasticsearch = Elasticsearch()\n",
    "    index_name: str = 'covid'\n",
    "\n",
    "    def drop(self):\n",
    "        self.client.indices.delete(index=self.index_name, ignore=[404])\n",
    "        return self\n",
    "\n",
    "    def create_index(self):\n",
    "        self.client.indices.create(index=self.index_name, body=self.index_file)\n",
    "        return self\n",
    "\n",
    "    def create_and_bulk_documents(self, entries:list):\n",
    "        entries_elastic = []\n",
    "        for entry in entries:\n",
    "            entry_elastic = {\n",
    "                **entry,\n",
    "                **{\n",
    "                    '_op_type': 'index',\n",
    "                    '_index': self.index_name\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            entries_elastic.append(entry_elastic)\n",
    "            \n",
    "        bulk(self.client, entries_elastic)\n",
    "\n",
    "    def __call__(self, entries: list):\n",
    "        self.create_and_bulk_documents(entries)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the work is done in `create_and_bulk_documents` where we just deconstruct one entry at the time and add two elastic search parameters.\n",
    "\n",
    "Unfortunately, Elastic Search won't be able to serialize the `numpy` arrays. So we need to create an adapter for our data. This class takes as input the paper data and the embedding and \"adapt\" them to work in our `ElasticSearchProvider`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CovidPapersEmbeddedAdapter:\n",
    "        \n",
    "    def __call__(self, x, embs):\n",
    "        for el, emb in zip(x, embs):\n",
    "            el['embed'] = np.array(emb).tolist()\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have everything in place. A way to represent the data, one to encode it in a vector and a method to store the result. Let's wrap everything up and encode all the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=128, num_workers=4, collate_fn=lambda x: x)\n",
    "es_adapter = CovidPapersEmbeddedAdapter()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(pr.base_dir / 'es_index.json', 'r') as f:\n",
    "    index_file = json.load(f)\n",
    "    es_provider = ElasticSearchProvider(index_file)\n",
    "    \n",
    "# drop the dataset\n",
    "es_provider.drop()\n",
    "# create a new one\n",
    "es_provider.create_index()\n",
    "\n",
    "for batch in tqdm(dl):\n",
    "    x = [b['title_abstract'] for b in batch]\n",
    "    embs = embedder(x)\n",
    "    es_provider(es_adapter(batch, embs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two tricks here, first, we use `torch.utils.data.DataLoader` to create a batch-wise iterator. In general, feeding data to the model in batch rather than as a single point boost the performance (in my case x100). Second, we replace the `collate_fn` parameter in the `DataLoader` constructor. This is because, by default, Pytorch will try to cast all our data into a `torch.Tensor` but it will fail to convert strings. By doing so, we just return an array of dictionaries, the output from `CovidPapersDataset`. So, a `batch` is a list of dictionaries with length `batch_size`. After we finished (~7m on a 1080ti), we can take a look at `http://localhost:9200/covid/_search?pretty=true&q=*:*`.\n",
    "\n",
    "If everything works correctly, you should see our data displayed by elastic search\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/medium/images/es_stored.jpg?raw=true)\n",
    "\n",
    "### Make a query\n",
    "\n",
    "We are almost done. The last piece of the puzzle is a way to search in the database. Elastic search can perform cosine similarity between one input vector and a target vector field in all the documents. The syntax is very straightforward:\n",
    "\n",
    "```\n",
    " {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "    \"script\": {\n",
    "        \"source\":\n",
    "        \"cosineSimilarity(params.query_vector, doc['embed']) + 1.0\",\n",
    "        \"params\": {\n",
    "            \"query_vector\": vector\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Where `vector` is our input. So, we created a class that takes a vector as an input and show all the results from the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ElasticSearcher:\n",
    "    \"\"\"\n",
    "    This class implements the logic behind searching for a vector in elastic search.\n",
    "    \"\"\"\n",
    "    client: Elasticsearch = Elasticsearch()\n",
    "    index_name: str = 'covid'\n",
    "\n",
    "    def __call__(self, vector: list):\n",
    "        script_query = {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, doc['embed'])\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": vector\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        res = self.client.search(\n",
    "            index= self.index_name,\n",
    "            body={\n",
    "                \"size\": 25,\n",
    "                \"query\": script_query,\n",
    "                \"_source\": {\n",
    "                    \"includes\": [\"title\", \"abstract\"]\n",
    "                }\n",
    "            })\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first result (I have copied and pasted the first matching paper's abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ElasticSearcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-976320c4364b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mes_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticSearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mes_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Effect of the virus on pregnant women'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ElasticSearcher' is not defined"
     ]
    }
   ],
   "source": [
    "es_search = ElasticSearcher()\n",
    "es_search(embedder(['Effect of the virus on pregnant women'])[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*As public health professionals respond to emerging infections, particular attention needs to be paid to **pregnant women** and their offspring. Pregnant women might be more susceptible to, or more severely affected by, emerging infections. The effects of a new maternal infection on the embryo or fetus are difficult to predict. Some medications recommended for prophylaxis or treatment could harm the embryo or fetus. We discuss the challenges of responding to emerging infections among pregnant women, and we propose strategies for overcoming these challenges.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! I have also created a command-line where the user can type a query. The final result is:\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/medium/images/cl.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More queries\n",
    "\n",
    "Finally, we can ask a question and find interesting papers. Empirically, queries with more details work better since they provide more context.\n",
    "\n",
    "For example, we may want to know the *What is the effectiveness of chloroquine for COVID-19*. The results are\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/medium/images/2.png?raw=true)\n",
    "\n",
    "Or *How does COVID-19 bind to the ACE2 receptor?*\n",
    "\n",
    "![alt](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/medium/images/3.png?raw=true)\n",
    "\n",
    "The search engine seems to work good but it is not perfect, in the next part of this tutorial we will try to improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this project, we build a semantic browser to search on more than 50k COVID-19 papers.The original project in which I have been working with students from the Universita of Trieste is [here](https://github.com/gsarti/covid-papers-browser). A live demo is available [here](http://covidbrowser.areasciencepark.it/)\n",
    "\n",
    "You can also play around with the command line app, you need to follow the instruction from [here](https://github.com/FrancescoSaverioZuppichini/Search-COVID-papers-with-Deep-Learning/blob/develop/README.md).\n",
    "\n",
    "\n",
    "#### Acknowledgment\n",
    "\n",
    "I would like to thank [Gabriele Santi](https://www.linkedin.com/in/gabrielesarti/) for helping me in the writing of this article, [Marco Franzon](https://www.linkedin.com/in/marco-franzon/) and [Tommaso Rodani](https://www.linkedin.com/in/tommaso-rodani-471a43b8/) for supporting me in the elastic search implementation.\n",
    "\n",
    "Thank you for reading\n",
    "\n",
    "Be safe,\n",
    "\n",
    "Francesco Saverio Zuppichini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
